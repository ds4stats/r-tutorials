---
title: "Introduction to Data scraping with R"
output:
  html_document:
    fig_height: 2
    fig_width: 5
  pdf_document: default
  word_document:
    fig_height: 2
    fig_width: 5
---
###Scraping Data From the Web###


Data on the web are often presented in tables. For instance, we can see a list of countries by population in 1900 on [Wikipedia](https://en.wikipedia.org/wiki/List_of_countries_by_population_in_1900)


Web pages are written in HTML (Hyper Text Markup Language) which uses **tags** to describe different aspects of document content. For example, a heading in a document is indicated by `<h1>My Title</h1>` whereas a paragraph would be indicated by `<p>A paragraph of content...</p>`. 


In this tutorial, we will learn how to read data from a table on a web page into R. We will need the package `rvest` to get the data from the web page, and the `stringr` package to clean up the data.


```{r, message = FALSE, warning = FALSE}
library(rvest)
library(stringr)
```


### 1. Reading data into R with `rvest`


To get the population data on [Wikipedia](https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population) into R, we use the `read_html` command from the `xml2` package (which is attached when `rvest` is called) to parse the page to obtain an HTML document. 


We then use the `html_nodes` command that extracts all occurrences of the desired tag. We will be interested in scraping data presented in tables, so in the source code, we look for the table tag: `<table> ... </table>`.


Note: some of the `rvest` commands may be slow depending on your Internet connection and the complexity of the web page.


```{r}
popParse <- read_html("https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population")

str(popParse)
```


The `read_html` command creates an R object, basically a list, that stores information about the web page.


To extract a table from this web page (this may take some time):


```{r}
popNodes <- html_nodes(popParse, "table")
popNodes
```
There are two tables in this document. By inspecting the output of `popNodes`, we make a guess that we want the first table. (In other cases, trial and error may be required.) We select the first table by using double brackets, which are used to index list objects:


```{r}
pop <- html_table(popNodes, header = TRUE, fill = TRUE)[[1]]
str(pop)

```

### 3. Cleaning the data frame


We now have a workable data frame that we can analyze. Notice that even though several of the columns are numbers, they are classified as "character." The `Population` column is also a character because the numbers have commas in them, plus some observations include characters such as `[1]` to indicate some footnotes. We need to convert these columns to be numeric.


We will also simplify the name of the third column to "Population."


```{r}
pop2 <- pop[-242, ]         #remove the last row (Total)
names(pop2) <- c("Rank", "Country", "Population", "Percent", "Date", "Source")
head(pop2)
```

To remove the commas in the Population numbers, we will use `str_replace_all` from the `stringr` package.

```{r}
pop2$Rank <- as.numeric(pop2$Rank) #coerce Rank to numeric
pop3 <- pop2
pop3$Population <- str_replace_all(pop2$Population, "\\$|,", "" )
pop3$Percent <- str_replace_all(pop2$Percent, "\\%", "" )
pop3$Country <- str_replace_all(pop2$Country, "\\[*.]", "" )
head(pop3)
```

### 4. A movie box office example


The web site [Box Office Mojo](http://www.boxofficemojo.com) gives statistics on box office earnings of movies. In addition to daily earnings, the web site also maintains lists of yearly and all time record holders.


We will look at the movies in the top 100 in all time movie worldwide grosses in box office receipts. In particular, we will scrape the data from [Box Office Mojo: All Time Box Office](http://www.boxofficemojo.com/alltime/world/?pagenum=1). The dollar amounts are in millions of dollars and the years marked with "^" indicate that the movie had multiple releases.


```{r}
movieParse<- read_html("http://www.boxofficemojo.com/alltime/world/?pagenum=1")

movieTables <- html_nodes(movieParse, "table")
head(movieTables)

movies1 <- html_table(movieTables, header = TRUE, fill = TRUE)[[1]]
names(movies1) <- c("Rank", "Title", "LifetimeGross", "Year") 
str(movies1)
```

### 5. Non-table Nodes
So far the two examples we've looked at extract tables from a webpage, but lots of other information from a webpage can be extracted using `rvest`.  The example below extracts data on the images contained on the Box Office Mojo webpage:
```{r}
movieParse<- read_html("https://www.boxofficemojo.com/")
moviesImg <- html_nodes(movieParse, "img")
head(moviesImg)
```
We see that there are 6 images on the page.  We can also see that the page has Twitter account items we might consider extracting using string processing. In general, there are a huge variety of nodes that we could extract, the next section provides a more detailed example of extracting data from a non-table html node.

### 6. A NY Times Article Example
Sometimes we might want to extract specific nodes that are defined by CSS (Cascading Style Sheets, a langauge which defines the style of an html page). For this goal, it can be very useful to view to html source code of a page  for guidance.
To view the HTML code behind a webpage, right click anywhere on the page and select "View Page Source" in Chrome or Firefox, "View Source" in Internet Explorer, or "Show Page Source" in Safari. (If that option doesn't appear in Safari, just open Safari Preferences, select the Advanced tab, and check "Show Develop menu in menu bar".)
The example we will look at is a [New York Times article](https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html).  The goal for this example is transform the body of text into a clean data.frame containing:
1. The lie itself
2. The date of that lie
3. A url sourcing the lie
By inspecting the html source code, we see that each lie has the following format:
```
<span class="short-desc"><strong> DATE </strong> LIE <span class="short-truth"><a href="URL"> EXPLANATION </a></span></span>
```
This tells us that extracting all `<span>` tags belonging to the class "short-desc" will provide us what we need.
```{r}
NytTrump <- read_html("https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html")
lies <- html_nodes(NytTrump, ".short-desc")
lies
```
The "." in from of "short-desc" is [CSS selector syntax](https://www.w3schools.com/cssref/css_selectors.asp), it will select all elements with class="short-desc".  See [CSS selector link](https://www.w3schools.com/cssref/css_selectors.asp) for more selector examples.
After extracting each lie, the first task is to find the date of each.  For this we notice that each date is wrapped within a `<strong>` tag.  Thus, extracting nodes with the `<strong>` tag will provide the dates we need:
```{r}
dates <- html_nodes(lies, "strong")
dates <- html_text(dates, trim = TRUE)  ## html_text will parse text from the html jargon
head(dates)
```
Extracting each lie requires a different strategy.  We could do it using regular expressions, but instead we use the `xml_contents` function:
```{r}
lies_var <- xml_contents(lies)
```
We see that each lie has three components, the second containing the lie itself, and these components are arranged in a vector. We can exploit this ordering and extract the lies using their positions:
```{r}
lies_var2 <- lies_var[seq(2, length(lies_var), by = 3)]
```
We can use a similar strategy to get the urls:
```{r}
lies_var3 <- lies_var[seq(3, length(lies_var), by = 3)]
```
From here we see that all the urls are tagged with "a", we can select these nodes using the tag, and then select the href object to get the url itself.  Note that we could have noticed this right away and eliminated the prior step.
```{r}
lies_var3 <- html_node(lies_var3, "a")
url_var <- html_attr(lies_var3, "href")
```
At this point we've satisfied the goal of this example, next step might be a textual analysis that further extracts key words or categorizes the urls.
```{r}
lies_clean <- data.frame(date = dates, lie = as.character(lies_var2), url = url_var)
head(lies_clean)
```

### 7.  A Billboard Top 100 example

The website [billboard.com](http://www.billboard.com)
keeps track of top songs, albums and artists from the music industry.


One page lists the greatest hot women artists. In the source code, here is one artist's listing:
`<div class="chart-list-item  " data-rank="24" data-artist="" data-title="Dionne Warwick" data-has-content="false">`.

(Note that the current number 1 entry, Madonna, is not listed in this format)

```{r}
webParse <- read_html("https://www.billboard.com/charts/greatest-hot-100-women-artists")
str(webParse)

webNodes <- html_nodes(webParse,".chart-list-item  " ) 
webNodes
```

We now need to extract the name of the artist, given in the `data-title` attribute:
```{r}
webNodes2 <- html_attr(webNodes, "data-title")
webNodes2
```

As we noted earlier, Madonna's entry was not listed in the same format as the others, so we will have to add her manually:

```{r}
top50women <- data.frame(top = c("Madonna", webNodes2))
head(top50women, 5)
```

### On Your Own

1. In the population table, why are the number of rows not equivalent to the highest rank value?
2. Create a new population table using a list of countries by population in 1900 on [Wikipedia](https://en.wikipedia.org/wiki/List_of_countries_by_population_in_1900).
3. Clean the movies1 data and create a graph with Year on the x-axis and LifetimeGross
on the y-axis.

#### Option 1
4. The web site [BikeRaceInfo](http://www.bikeraceinfo.com/tdf/tdfstats.html) has a table with data on past winners of the Tour de France. Create a cleaned-up data frame of this data.

#### Option 2
4. The web site [NY Times Best Sellers: Hardcover Fiction](http://www.nytimes.com/books/best-sellers/hardcover-fiction) contains a list of best-selling fiction books. Scrape the names of these top books and store the results in a clean data.frame. The list of books are tagged via `<h2>title</h2>`.

#### Option 3
4. The link https://www.auhouseprices.com/rent/list/NSW/2131/Ashfield/1/?sort=date&type=apartment&bmin=2&bmax=2 shows two-bedroom rental listings in Ashfield Australia.  Scrap this webpage to create a clean data.frame containing three variables: each rental's address, its price, and the month/year when the price was offered. Hint: this is best accomplished by writing a function and using a *for loop*.
```{r}
N <- 3
for(i in 1:N){
  print(i)
}
```
### Resources

* [HTML Tutorial](https://www.w3schools.com/html/)
* [Selector Gadget](https://selectorgadget.com/). A nice example is [Scraping HomePrices](https://www.youtube.com/watch?v=JAPDCTHG-zc)
